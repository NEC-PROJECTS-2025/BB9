# -*- coding: utf-8 -*-
"""final_project3main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QJC1SPTODXwAXgG2Bfsjdq8TQTqmMqOW

## import modules
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation
from tensorflow.keras.optimizers import SGD, Adagrad, Adadelta, RMSprop, Adam
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
import warnings
warnings.simplefilter('ignore')

from matplotlib.pyplot import imshow
from keras.preprocessing import image
from keras import applications
import os
import glob

import cv2
import warnings
warnings.simplefilter('ignore')

import keras
from keras import layers
from keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D
from keras.layers import AveragePooling2D,GlobalAveragePooling2D,GlobalMaxPool2D,MaxPooling2D,MaxPool2D,Dropout
from keras.models import Model,Sequential

"""## Loading the dataset"""

from google.colab import drive
drive.mount('/content/drive')

!unzip '/content/drive/MyDrive/project/archive (1).zip'

os.listdir('/content/cropped')

glob.glob('/content/cropped/*')

glob.glob('/content/cropped/cropped/*')

glob.glob('/content/cropped/cropped/train/n02112706-Brabancon_griffon/*')

"""## Analyze and Visualize the Dataset"""

class_dir1=os.listdir('/content/cropped/cropped/test/')
class_dir1

dog_names1 = [file.split('-')[1] for file in class_dir1]
dog_names1

import random
import glob
import tensorflow as tf

count_dict1 = {}
img_dict1 = {}

# Loop through classes
for cls in class_dir1:  # Assuming class_names contains the list of dog classes
    image_path = glob.glob(f'/content/cropped/cropped/test/{cls}/*')
    count_dict1[cls] = len(image_path)

    if image_path:  # Check if image_path is not empty
        img_dict1[cls] = tf.keras.utils.load_img(random.choice(image_path))
count_dict1

df1 = pd.DataFrame(data={'label':count_dict1.keys(),'count':count_dict1.values()})
df1 = df1[df1['count'] != 0]
df1

labels=list(df1['label'])
labels

img = cv2.imread('/content/cropped/cropped/train/n02085782-Japanese_spaniel/n02085782_1059.jpg')
print(img.shape)
img1 = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
plt.imshow(img1)

img = cv2.imread('/content/cropped/cropped/test/n02085936-Maltese_dog/n02085936_10130.jpg')
print(img.shape)
img1 = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
plt.imshow(img1)

import math

num_items = len(img_dict1)
num_cols = 4
num_rows = math.ceil(num_items / num_cols)

plt.figure(figsize=(20, 5 * num_rows))  # Adjust the figure size based on the number of rows

for id, (label, img) in enumerate(img_dict1.items()):
    plt.subplot(num_rows, num_cols, id + 1)
    plt.imshow(img)
    plt.title(f"{label.split('-')[1]} {img.size}")
    plt.axis('off')

df1['label']=[file.split('-')[1] for file in labels]
plt.figure(figsize=(20,8))
sns.barplot(x='label',y='count',data=df1)
plt.xticks(rotation=90)
plt.show()

"""## Preprocessing"""

train_datagen =  ImageDataGenerator(rescale=1./255,
                                   shear_range=0.15,
                                   rotation_range=20,
                                   width_shift_range=0.1,
                                   zoom_range=0.2,
                                   horizontal_flip=True,
                                   fill_mode='nearest',
                                   height_shift_range=0.1
                                   )
test_datagen = ImageDataGenerator(rescale=1./255)


train_set = train_datagen.flow_from_directory('/content/cropped/cropped/train',
                                              target_size=(224,224),
                                              batch_size=32,
                                              class_mode='categorical'
                                             )

test_set = train_datagen.flow_from_directory('/content/cropped/cropped/test',
                                              target_size=(224,224),
                                              batch_size=32,
                                              class_mode='categorical'
                                             )
num_classes = len(train_set.class_indices)
print("Number of classes:", num_classes)

train_set.class_indices

# Calculate class weights
y_train = train_set.classes
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights = dict(enumerate(class_weights))
print(class_weights)



"""## Xecption"""

# Import libaries
from keras.applications import Xception
from keras.optimizers import RMSprop
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
# from keras.preprocessing.image import ImageDataGenerator

# Create Xception base model
from keras.applications.xception import Xception
base_model_xception = Xception(weights='imagenet', include_top=False, input_shape=(224,224,3))

# Assuming you have a pre-trained Xception model as base_model_xception
model_xception = Sequential()
model_xception.add(base_model_xception)
model_xception.add(GlobalAveragePooling2D())
model_xception.add(Dense(1024, activation='relu'))
model_xception.add(Dropout(0.5))
model_xception.add(Dense(120, activation='softmax'))

# Define RMSprop optimizer with specific parameters
optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-07)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-7)

# Compile the model with the RMSprop optimizer
model_xception.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy','Precision','Recall'])



from keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history=model_xception.fit(train_set,epochs=10,validation_data=test_set,callbacks=[early_stopping, reduce_lr])

model_xception.save('xception_model.h5') # Changed 'model' to 'model_xception' to save the correct model

# Plot training and validation accuracy/loss
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Accuracy/Loss')
plt.xlabel('Epoch')
plt.ylabel('Accuracy/Loss')
plt.legend()
plt.show()

"""## NASnetmobile"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import NASNetMobile
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt

base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Add custom layers on top of the base model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(train_set.num_classes, activation='softmax')(x)

# Combine the base model with the custom layers
model = Model(inputs=base_model.input, outputs=predictions)

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_nasnetmobile_model.h5.keras', monitor='val_loss', save_best_only=True)

history = model.fit(
    train_set,
    epochs=10,
    validation_data=test_set,
    callbacks=[reduce_lr, early_stopping, model_checkpoint]
)

model.save('NANSNET_model.h5') # Changed 'model_NASNET' to 'model' to save the correct model

# Plot training and validation accuracy/loss
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Accuracy/Loss')
plt.xlabel('Epoch')
plt.ylabel('Accuracy/Loss')
plt.legend()
plt.show()

"""## Inception ResNetv2"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Load InceptionResNetV2 model pre-trained on ImageNet
base_model = tf.keras.applications.InceptionResNetV2(
    include_top=False,
    weights='imagenet',
    input_shape=(224,224,3)
)

# Freeze the base model layers
base_model.trainable = False

# Add classification head to the base model
model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(120, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

input_shape = (224, 224, 3)
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=input_shape),
    model
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])



model.summary()

epochs = 10

# Check if test_set is empty
if len(test_set) > 0:
    history = model.fit(
        train_set,
        steps_per_epoch=len(train_set),
        epochs=epochs,
        validation_data=test_set,
        # Set validation_steps to 1 to ensure at least one validation step per epoch
        validation_steps=1
    )
else:
    print("Error: test_set is empty. Please check your data loading.")

model.save('INCEPTION_RESNETV2.h5') # Changed 'model_NASNET' to 'model' to save the correct model

val_loss, val_acc = model.evaluate(test_set, steps=len(test_set))
print(f"Validation accuracy: {val_acc:.4f}")

# Plot training and validation accuracy/loss
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Accuracy/Loss')
plt.xlabel('Epoch')
plt.ylabel('Accuracy/Loss')
plt.legend()
plt.show()

"""## Inception V3"""

from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import pandas as pd
import numpy as np
from tensorflow.keras.models import Model,load_model
from glob import glob
from tensorflow.keras.preprocessing import image
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense,Activation,GlobalAveragePooling2D
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf

inception_model =InceptionV3(include_top=False,input_shape=[224,224,3])

for layer in inception_model.layers:
    layer.trainable = False

x =GlobalAveragePooling2D()(inception_model.output)

x = Dense(120)(x)

pred = Activation('softmax')(x)

model = Model(inputs=inception_model.input,outputs=pred)
model.summary()

checkpoint = ModelCheckpoint('smart_model_mobile.keras',monitor='val_accuracy',verbose=1,save_best_only=True) # Changed the file extension to .keras
callback = [checkpoint]

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

history = model.fit(train_set,epochs=10,steps_per_epoch=len(train_set),validation_data=test_set,verbose=1,callbacks=callback)

model.save('INCEPTIONV3_model.h5') # Changed 'model_NASNET' to 'model' to save the correct model

# Plot training and validation accuracy/loss
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Accuracy/Loss')
plt.xlabel('Epoch')
plt.ylabel('Accuracy/Loss')
plt.legend()
plt.show()

"""## Result Graph"""

import matplotlib.pyplot as plt
import numpy as np

# Example data for accuracies of 4 models
models = [' Xception', 'NasNet Mobile', 'Inception ResNetv2', 'InceptionV3']
train_accuracies = [82.37, 82.03, 82.08, 87.25]  # Training accuracies for each model
val_accuracies = [65.37, 83.22, 84.92, 79.69]    # Validation accuracies for each model

# Set the position of the bars on the x-axis
x = np.arange(len(models))  # the label locations
width = 0.35  # the width of the bars

# Create the plot
fig, ax = plt.subplots(figsize=(8, 8))
bar1 = ax.bar(x - width/2, train_accuracies, width, label='Training Accuracy', color='skyblue')
bar2 = ax.bar(x + width/2, val_accuracies, width, label='Validation Accuracy', color='salmon')

# Adding labels and title
ax.set_xlabel('Model names')
ax.set_ylabel('Accuracy Percentage')
ax.set_title('Accuracy Comparison of 4 Models')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()

# Display the accuracies on top of the bars
for bar in bar1:
    height = bar.get_height()
    ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')

for bar in bar2:
    height = bar.get_height()
    ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')

# Show the plot
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""## predict"""

from tensorflow.keras.preprocessing import image
import numpy as np

def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(150, 150))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0  # Rescale the image
    return img_array

# Example image path (update with the actual image path)
img_path = '/content/cropped/cropped/train/n02085620-Chihuahua/n02085620_10621.jpg'  # Update with the path to the image you want to predict

# Preprocess the image
img_array = preprocess_image(img_path)

# Make predictions using each trained model
for opt_name, model in trained_models.items():
    predictions = model.predict(img_array)
    predicted_class = np.argmax(predictions, axis=1)

    class_labels = {v: k for k, v in train_data.class_indices.items()}  # Get class labels from the training data
    predicted_label = class_labels[predicted_class[0]]

    print(f'Predicted class with {opt_name} optimizer: {predicted_label}')

from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np

# Function to preprocess the image
def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0  # Rescale the image
    return img_array

# Example image path (update with your actual image path)
img_path = '/content/cropped/cropped/train/n02085782-Japanese_spaniel/n02085782_1059.jpg'
# Preprocess the image
img_array = preprocess_image(img_path)

# Load the saved model
model_path = '/content/NANSNET_model.h5'  # Replace with the correct model file name if different
model = load_model(model_path)

# Make predictions using the loaded model
predictions = model.predict(img_array)
predicted_class = np.argmax(predictions, axis=1)

# Get class labels from the training data
class_labels = {v: k for k, v in train_set.class_indices.items()}  # Replace train_set with the appropriate variable
predicted_label = class_labels[predicted_class[0]]

print(f'Predicted class: {predicted_label}')

"""## Pratices example"""

from keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, Flatten, Dense
from keras.models import Sequential

def create_model():
  # Initialize the Sequential model
  model = Sequential()

  # First Convolutional Layer
  model.add(Conv2D(12, (3, 3), padding='same', input_shape=(224, 224, 3)))
  model.add(BatchNormalization())
  model.add(ReLU())
  model.add(MaxPooling2D(pool_size=(2, 2)))  # Reduces size to 12 × 75 × 75

  # Second Convolutional Layer
  model.add(Conv2D(20, (3, 3), padding='same'))
  model.add(BatchNormalization())
  model.add(ReLU())
  model.add(MaxPooling2D(pool_size=(2, 2)))  # Assuming no pooling for the second layer, size remains 20 × 75 × 75

  # Third Convolutional Layer
  model.add(Conv2D(32, (3, 3), padding='same'))
  model.add(BatchNormalization())
  model.add(ReLU())

  # Flatten the feature maps for the Fully Connected Layers
  model.add(Flatten())  # Converts 32 × 75 × 75 into a single 1D vector

  # Fully Connected Layers
  model.add(Dense(256, activation='relu'))  # Example dense layer for intermediate processing
  model.add(Dense(128, activation='relu'))  # Example dense layer for intermediate processing

  # Output Layer
  model.add(Dense(120, activation='softmax'))  # Output layer with 2 classes (healthy and unhealthy)

  return model

def train_model(optimizer, optimizer_name):
    model = create_model()
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    history = model.fit(
        train_data,
        epochs=25,
        validation_data=test_data,
        class_weight=class_weights
    )

    # Evaluate the model
    loss, accuracy = model.evaluate(test_data)
    print(f'Test loss with {optimizer_name}: {loss*100:.4f}')
    print(f'Test accuracy with {optimizer_name}: {accuracy*100:.4f}')


    highest_val_accuracy = max(history.history['val_accuracy'])
    print(f'Highest Validation Accuracy with {optimizer_name}: {highest_val_accuracy * 100:.2f}%')

    return model, history

# List of optimizers
optimizers = {
    #'SGD': SGD(learning_rate=0.001),
    #'Adagrad': Adagrad(learning_rate=0.01),
    #'Adadelta': Adadelta(learning_rate=0.01),
    #'RMSProp': RMSprop(learning_rate=0.001),
    'Adam': Adam(learning_rate=0.001)
}

# Train and evaluate the model with different optimizers
# histories = {}
# for opt_name, opt in optimizers.items():
#     print(f"Training with {opt_name} optimizer")
#     histories[opt_name] = train_model(opt, opt_name)

trained_models = {}
histories = {}
for opt_name, opt in optimizers.items():
    print(f"Training with {opt_name} optimizer")
    model, history = train_model(opt, opt_name)
    trained_models[opt_name] = model
    histories[opt_name] = history

def train_model(optimizer, optimizer_name):
    model = create_model()
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    history = model.fit(
        train_data,
        epochs=25,
        validation_data=test_data,
        class_weight=class_weights
    )

    # Evaluate the model
    loss, accuracy = model.evaluate(test_data)
    print(f'Test loss with {optimizer_name}: {loss*100:.4f}')
    print(f'Test accuracy with {optimizer_name}: {accuracy*100:.4f}')


    highest_val_accuracy = max(history.history['val_accuracy'])
    print(f'Highest Validation Accuracy with {optimizer_name}: {highest_val_accuracy * 100:.2f}%')

    return model, history

# List of optimizers
optimizers = {
    # 'SGD': SGD(learning_rate=0.001),
    # 'Adagrad': Adagrad(learning_rate=0.01),
    # 'Adadelta': Adadelta(learning_rate=0.01),
    # 'RMSProp': RMSprop(learning_rate=0.001),
    'Adam': Adam(learning_rate=0.0001)
}

# Train and evaluate the model with different optimizers
trained_models = {}
histories = {}
for opt_name, opt in optimizers.items():
    print(f"Training with {opt_name} optimizer")
    model, history = train_model(opt, opt_name)
    trained_models[opt_name] = model
    histories[opt_name] = history

import pandas as pd
import numpy as np

optimizers=np.array(['SGD Optimizer','Adagrad Optimizer','Adadelta Optimizer','RMSProp Optimizer','Adam Optimizer'])
test1= np.array(['89.36%','93.68%','87.37%','80.73%','100%'])
testt2=np.array(['89.70%','96.01%','96.68%','82.39%','100%'])

df=pd.DataFrame({'Optimizers':optimizers,'Base_Paper_Accuracy':test1,'Obtained_Accuracy':testt2})
df

